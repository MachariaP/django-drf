# üìò Django REST Framework - Next Steps Implementation Guide (Part 3)

> **Taking Your API to Enterprise Scale**: A comprehensive guide to implementing CI/CD, Kubernetes, API Gateway, Machine Learning, Search, and Analytics for production-ready Django REST Framework APIs.

---

## üìë Table of Contents

1. [Introduction](#1-introduction)
2. [CI/CD Implementation](#2-cicd-implementation)
3. [Kubernetes Deployment](#3-kubernetes-deployment)
4. [API Gateway Integration](#4-api-gateway-integration)
5. [Machine Learning Integration](#5-machine-learning-integration)
6. [Elasticsearch Search Implementation](#6-elasticsearch-search-implementation)
7. [Analytics Implementation](#7-analytics-implementation)
8. [Best Practices and Production Tips](#8-best-practices-and-production-tips)

---

## 1. Introduction

### What This Guide Covers

This guide builds upon the foundational knowledge from `guide.md` and `guide001.md` to implement enterprise-level features that transform your Django REST Framework API from a development project into a production-ready, scalable system.

### Prerequisites

Before starting, you should have:

‚úÖ A working Django REST Framework API (from guide.md)  
‚úÖ Understanding of models, serializers, and viewsets  
‚úÖ Basic knowledge of authentication and permissions  
‚úÖ Docker and containerization basics  
‚úÖ Understanding of REST API principles  

### What You'll Learn

By the end of this guide, you'll understand how to:

üöÄ **Automate Everything** - CI/CD pipelines for testing and deployment  
‚ò∏Ô∏è **Scale Infinitely** - Kubernetes orchestration for container management  
üö™ **Control Access** - API Gateway for routing, rate limiting, and security  
ü§ñ **Add Intelligence** - Machine learning for recommendations and predictions  
üîç **Enable Search** - Elasticsearch for fast, powerful search capabilities  
üìä **Track Everything** - Analytics for insights into API usage and performance  

---

## 2. CI/CD Implementation

### 2.1 What is CI/CD and Why It Matters

**Continuous Integration (CI)** is the practice of automatically testing code every time changes are pushed to the repository. **Continuous Deployment (CD)** automatically deploys tested code to production environments.

#### Why CI/CD is Critical

| Without CI/CD | With CI/CD | Business Impact |
|--------------|------------|-----------------|
| Manual testing takes hours/days | Automated tests run in minutes | **Faster feature delivery** |
| Bugs discovered in production | Bugs caught before deployment | **Better user experience** |
| Deployment fear (might break things) | Confident deployments | **More frequent releases** |
| Inconsistent environments | Identical builds every time | **Fewer production issues** |
| Manual deployment errors | Automated, repeatable process | **Reduced downtime** |

**Real-world example**: Without CI/CD, deploying might happen weekly or monthly due to fear and manual effort. With CI/CD, successful companies deploy **dozens of times per day** with confidence.

### 2.2 GitHub Actions CI/CD

GitHub Actions is built directly into GitHub, making it the easiest CI/CD solution for GitHub-hosted repositories.

#### How It Works

1. **Trigger**: Push code or create pull request
2. **Runner**: GitHub spins up a clean virtual machine
3. **Steps**: Install dependencies, run tests, build, deploy
4. **Result**: Pass/fail status, deployment to staging/production

#### Step 1: Create Workflow Directory

```bash
# Create GitHub Actions workflow directory
mkdir -p .github/workflows
```

#### Step 2: Create CI Workflow

Create `.github/workflows/ci.yml`:

```yaml
name: CI Pipeline

# When to run this workflow
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

# Jobs to run
jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    
    # Service containers (database, cache)
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: test_db
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v3
      
      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'  # Cache pip dependencies
      
      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install coverage pytest pytest-django
      
      # Step 4: Run linters
      - name: Run linters
        run: |
          pip install flake8 black isort
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          black --check .
          isort --check-only .
      
      # Step 5: Run tests with coverage
      - name: Run tests
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0
          SECRET_KEY: test-secret-key-for-ci
          DEBUG: False
        run: |
          coverage run --source='.' manage.py test
          coverage report --fail-under=80
          coverage xml
      
      # Step 6: Upload coverage reports
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: true

  security:
    name: Security Checks
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install safety bandit
      
      # Check for known security vulnerabilities in dependencies
      - name: Check dependencies for vulnerabilities
        run: safety check --json
      
      # Static code analysis for security issues
      - name: Run Bandit security scanner
        run: bandit -r . -f json -o bandit-report.json
      
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            bandit-report.json
```

**Understanding the Workflow**:

| Section | Purpose | Why It's Important |
|---------|---------|-------------------|
| **on: push/pull_request** | Trigger events | Ensures every code change is automatically tested before merging |
| **services** | Spin up databases/cache | Tests run against real services, catching integration bugs |
| **checkout** | Get the code | GitHub Actions needs your code to test it |
| **cache: 'pip'** | Cache Python packages | Speeds up builds from ~5 minutes to ~1 minute by reusing packages |
| **flake8/black/isort** | Code quality checks | Maintains consistent code style across team, catches common errors |
| **coverage** | Test coverage reporting | Ensures new code is tested; fails if coverage drops below 80% |
| **safety/bandit** | Security scanning | Catches known vulnerabilities and insecure code patterns early |

#### Step 3: Create CD Workflow

Create `.github/workflows/deploy.yml`:

```yaml
name: CD Pipeline

on:
  push:
    branches: [ main ]
  workflow_dispatch:  # Allow manual deployment

jobs:
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    environment: production  # Requires approval for production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run migrations (dry-run)
        env:
          DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
        run: |
          python manage.py migrate --check
      
      - name: Build Docker image
        run: |
          docker build -t ${{ secrets.DOCKER_REGISTRY }}/django-api:${{ github.sha }} .
          docker tag ${{ secrets.DOCKER_REGISTRY }}/django-api:${{ github.sha }} \
                     ${{ secrets.DOCKER_REGISTRY }}/django-api:latest
      
      - name: Push to Docker Registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push ${{ secrets.DOCKER_REGISTRY }}/django-api:${{ github.sha }}
          docker push ${{ secrets.DOCKER_REGISTRY }}/django-api:latest
      
      - name: Deploy to Kubernetes
        uses: azure/k8s-deploy@v4
        with:
          manifests: |
            k8s/deployment.yml
            k8s/service.yml
          images: |
            ${{ secrets.DOCKER_REGISTRY }}/django-api:${{ github.sha }}
          kubectl-version: 'latest'
      
      - name: Notify deployment
        if: success()
        run: |
          curl -X POST ${{ secrets.SLACK_WEBHOOK }} \
            -H 'Content-Type: application/json' \
            -d '{"text":"‚úÖ Deployment successful: ${{ github.sha }}"}'
      
      - name: Rollback on failure
        if: failure()
        run: |
          kubectl rollout undo deployment/django-api
          curl -X POST ${{ secrets.SLACK_WEBHOOK }} \
            -H 'Content-Type: application/json' \
            -d '{"text":"‚ùå Deployment failed, rolled back: ${{ github.sha }}"}'
```

**Deployment Workflow Explained**:

- **environment: production** - Requires manual approval before deploying to production (safety gate)
- **migrate --check** - Verifies migrations will work without actually running them
- **Docker build** - Creates container image tagged with git commit SHA for traceability
- **Push to registry** - Stores image in Docker Hub, AWS ECR, or Google Container Registry
- **Deploy to Kubernetes** - Updates Kubernetes cluster with new image
- **Notifications** - Sends success/failure alerts to Slack or email
- **Rollback** - Automatically reverts to previous version if deployment fails

### 2.3 GitLab CI/CD

GitLab provides built-in CI/CD with `.gitlab-ci.yml`.

Create `.gitlab-ci.yml`:

```yaml
# Define stages of the pipeline
stages:
  - test
  - build
  - deploy

# Global variables
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  POSTGRES_DB: test_db
  POSTGRES_USER: test_user
  POSTGRES_PASSWORD: test_password

# Cache pip packages between jobs
cache:
  paths:
    - .cache/pip
    - venv/

# Template for Python jobs
.python_template: &python_template
  image: python:3.11
  before_script:
    - python -m venv venv
    - source venv/bin/activate
    - pip install -r requirements.txt

# Test job
test:
  <<: *python_template
  stage: test
  services:
    - postgres:15
    - redis:7-alpine
  script:
    - flake8 .
    - black --check .
    - pytest --cov=. --cov-report=xml --cov-report=term
  coverage: '/TOTAL.*\s+(\d+%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

# Security scanning
security:
  <<: *python_template
  stage: test
  script:
    - pip install safety bandit
    - safety check
    - bandit -r . -f json -o bandit-report.json
  artifacts:
    paths:
      - bandit-report.json

# Build Docker image
build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA $CI_REGISTRY_IMAGE:latest
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - docker push $CI_REGISTRY_IMAGE:latest
  only:
    - main
    - develop

# Deploy to staging
deploy_staging:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context staging
    - kubectl set image deployment/django-api django-api=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - kubectl rollout status deployment/django-api
  environment:
    name: staging
    url: https://staging.api.example.com
  only:
    - develop

# Deploy to production (manual trigger)
deploy_production:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context production
    - kubectl set image deployment/django-api django-api=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - kubectl rollout status deployment/django-api
  environment:
    name: production
    url: https://api.example.com
  when: manual
  only:
    - main
```

**GitLab CI/CD Advantages**:

- **Built-in Container Registry**: No need for external Docker registry
- **Environments**: Built-in environment management and deployment tracking
- **Review Apps**: Automatically create temporary environments for each merge request
- **Security Scanning**: Built-in SAST, DAST, dependency scanning
- **Pipeline Visualization**: Beautiful UI showing pipeline progress

### 2.4 Jenkins Pipeline

Jenkins is a self-hosted CI/CD server offering maximum flexibility.

Create `Jenkinsfile`:

```groovy
pipeline {
    agent any
    
    environment {
        DOCKER_REGISTRY = credentials('docker-registry')
        KUBECONFIG = credentials('kubernetes-config')
        SLACK_WEBHOOK = credentials('slack-webhook')
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }
        
        stage('Setup Python') {
            steps {
                sh '''
                    python3 -m venv venv
                    . venv/bin/activate
                    pip install --upgrade pip
                    pip install -r requirements.txt
                '''
            }
        }
        
        stage('Lint & Format Check') {
            parallel {
                stage('Flake8') {
                    steps {
                        sh '''
                            . venv/bin/activate
                            flake8 . --count --statistics
                        '''
                    }
                }
                stage('Black') {
                    steps {
                        sh '''
                            . venv/bin/activate
                            black --check .
                        '''
                    }
                }
                stage('isort') {
                    steps {
                        sh '''
                            . venv/bin/activate
                            isort --check-only .
                        '''
                    }
                }
            }
        }
        
        stage('Test') {
            steps {
                sh '''
                    . venv/bin/activate
                    coverage run --source='.' manage.py test
                    coverage report
                    coverage xml
                '''
            }
            post {
                always {
                    publishCoverage adapters: [coberturaAdapter('coverage.xml')]
                }
            }
        }
        
        stage('Security Scan') {
            parallel {
                stage('Safety Check') {
                    steps {
                        sh '''
                            . venv/bin/activate
                            safety check --json
                        '''
                    }
                }
                stage('Bandit') {
                    steps {
                        sh '''
                            . venv/bin/activate
                            bandit -r . -f json -o bandit-report.json
                        '''
                    }
                }
            }
        }
        
        stage('Build Docker Image') {
            when {
                branch 'main'
            }
            steps {
                script {
                    def commitHash = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim()
                    sh """
                        docker build -t ${DOCKER_REGISTRY}/django-api:${commitHash} .
                        docker tag ${DOCKER_REGISTRY}/django-api:${commitHash} ${DOCKER_REGISTRY}/django-api:latest
                    """
                }
            }
        }
        
        stage('Push to Registry') {
            when {
                branch 'main'
            }
            steps {
                script {
                    def commitHash = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim()
                    sh """
                        docker push ${DOCKER_REGISTRY}/django-api:${commitHash}
                        docker push ${DOCKER_REGISTRY}/django-api:latest
                    """
                }
            }
        }
        
        stage('Deploy to Staging') {
            when {
                branch 'develop'
            }
            steps {
                sh '''
                    kubectl config use-context staging
                    kubectl apply -f k8s/
                    kubectl rollout status deployment/django-api
                '''
            }
        }
        
        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            steps {
                input message: 'Deploy to production?', ok: 'Deploy'
                sh '''
                    kubectl config use-context production
                    kubectl apply -f k8s/
                    kubectl rollout status deployment/django-api
                '''
            }
        }
    }
    
    post {
        success {
            slackSend(
                color: 'good',
                message: "‚úÖ Pipeline SUCCESS: ${env.JOB_NAME} ${env.BUILD_NUMBER}",
                webhookUrl: "${SLACK_WEBHOOK}"
            )
        }
        failure {
            slackSend(
                color: 'danger',
                message: "‚ùå Pipeline FAILED: ${env.JOB_NAME} ${env.BUILD_NUMBER}",
                webhookUrl: "${SLACK_WEBHOOK}"
            )
        }
    }
}
```

**Jenkins Features**:

- **Parallel Execution**: Run linters simultaneously to save time
- **Manual Approval**: Production deployment requires human approval
- **Post-build Actions**: Notifications regardless of pipeline result
- **Credentials Management**: Secure storage of secrets and tokens
- **Plugin Ecosystem**: Thousands of plugins for every integration imaginable

### 2.5 Best Practices for CI/CD

#### DO's ‚úÖ

| Practice | Benefit | Implementation |
|----------|---------|----------------|
| **Keep pipelines fast** | Developers get feedback in <10 minutes | Run tests in parallel, cache dependencies, use fast runners |
| **Fail fast** | Save time by stopping at first failure | Run linters before tests, quick checks before slow ones |
| **Use secrets management** | Prevent credential leaks | GitHub Secrets, GitLab Variables, Jenkins Credentials |
| **Test in production-like environments** | Catch environment-specific bugs | Use same OS, Python version, database as production |
| **Implement gradual rollout** | Reduce blast radius of bugs | Deploy to 10% of servers, monitor, then 100% |
| **Automate rollbacks** | Quick recovery from failures | Detect failure, automatically revert to previous version |

#### DON'Ts ‚ùå

| Anti-pattern | Problem | Better Approach |
|--------------|---------|-----------------|
| **Skip tests to save time** | Bugs reach production | Optimize tests to run faster, never skip them |
| **Manual deployment steps** | Human errors, slow process | Automate everything, use infrastructure as code |
| **Deploy without migrations** | Database and code mismatch | Run migrations as part of deployment pipeline |
| **Ignore test failures** | Technical debt accumulates | Fix immediately, block merges on test failures |
| **Commit secrets to repo** | Security vulnerability | Use environment variables and secret management |

---

## 3. Kubernetes Deployment

### 3.1 What is Kubernetes and Why Use It

**Kubernetes (K8s)** is a container orchestration platform that automates deployment, scaling, and management of containerized applications.

#### The Problem Kubernetes Solves

**Without Kubernetes**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Manual Container Management           ‚îÇ
‚îÇ  - Start/stop containers manually      ‚îÇ
‚îÇ  - Monitor each server individually    ‚îÇ
‚îÇ  - Manual load balancing               ‚îÇ
‚îÇ  - Manual scaling                      ‚îÇ
‚îÇ  - No automatic restart on failure     ‚îÇ
‚îÇ  - Complex networking setup            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**With Kubernetes**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Automated Orchestration               ‚îÇ
‚îÇ  ‚úÖ Self-healing: auto-restart failed  ‚îÇ
‚îÇ  ‚úÖ Auto-scaling: scale based on load  ‚îÇ
‚îÇ  ‚úÖ Load balancing: built-in           ‚îÇ
‚îÇ  ‚úÖ Rolling updates: zero-downtime     ‚îÇ
‚îÇ  ‚úÖ Service discovery: automatic       ‚îÇ
‚îÇ  ‚úÖ Secret management: built-in        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Real-World Example

Imagine you're running a bookstore API:

**Monday morning**: Normal traffic, 3 servers handle load easily  
**Friday afternoon**: Black Friday sale starts, traffic jumps 10x  
**Without K8s**: Manual scramble to start more servers, might take hours, customers see errors  
**With K8s**: Automatically detects high CPU, spins up 30 servers in minutes, handles traffic smoothly  

**Server crashes**:  
**Without K8s**: Alert fires, engineer woken up, manually restarts server (downtime: 30+ minutes)  
**With K8s**: Detects crash, automatically starts new container (downtime: <30 seconds)  

### 3.2 Kubernetes Core Concepts

Before diving into configuration, understand these key concepts:

| Concept | Simple Explanation | Django API Example |
|---------|-------------------|-------------------|
| **Pod** | Smallest deployable unit, contains one or more containers | Your Django app container + a logging sidecar container |
| **Deployment** | Manages desired state of pods (how many, which version) | "I want 5 replicas of Django app version 1.2.3" |
| **Service** | Stable network endpoint for accessing pods | Load balancer distributing traffic to Django pods |
| **ConfigMap** | Configuration data (non-sensitive) | Django settings like `ALLOWED_HOSTS`, `DEBUG=False` |
| **Secret** | Sensitive data (passwords, keys) | Database password, `SECRET_KEY`, API keys |
| **Ingress** | HTTP routing and SSL termination | Routes `api.example.com` to your service |
| **Namespace** | Virtual cluster for organizing resources | Separate `production` and `staging` environments |
| **PersistentVolume** | Storage that persists beyond pod lifecycle | PostgreSQL data, media file uploads |

### 3.3 Setting Up Kubernetes

#### Option 1: Local Development with Minikube

```bash
# Install Minikube (local Kubernetes cluster)
# On macOS
brew install minikube

# On Linux
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# Start Minikube
minikube start --cpus=4 --memory=8192

# Verify installation
kubectl get nodes
```

#### Option 2: Cloud Kubernetes Services

**Google Kubernetes Engine (GKE)**:
```bash
# Install gcloud CLI
curl https://sdk.cloud.google.com | bash

# Create GKE cluster
gcloud container clusters create django-cluster \
  --num-nodes=3 \
  --machine-type=n1-standard-2 \
  --zone=us-central1-a

# Get credentials
gcloud container clusters get-credentials django-cluster
```

**Amazon EKS**:
```bash
# Install eksctl
brew install eksctl  # macOS
# or download from https://github.com/weaveworks/eksctl

# Create EKS cluster
eksctl create cluster \
  --name django-cluster \
  --region us-east-1 \
  --nodegroup-name standard-workers \
  --node-type t3.medium \
  --nodes 3

# Get credentials (automatic with eksctl)
```

**Azure AKS**:
```bash
# Install Azure CLI
brew install azure-cli  # macOS

# Login to Azure
az login

# Create AKS cluster
az aks create \
  --resource-group myResourceGroup \
  --name django-cluster \
  --node-count 3 \
  --node-vm-size Standard_D2s_v3 \
  --enable-addons monitoring

# Get credentials
az aks get-credentials --resource-group myResourceGroup --name django-cluster
```

### 3.4 Kubernetes Configuration Files

Create a `k8s/` directory for Kubernetes manifests:

```bash
mkdir -p k8s
```

#### Step 1: Namespace

Create `k8s/namespace.yml`:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: django-api
  labels:
    name: django-api
    environment: production
```

**Why namespaces?** Isolate environments (production, staging, development) on the same cluster, preventing accidental changes to wrong environment.

#### Step 2: ConfigMap

Create `k8s/configmap.yml`:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: django-config
  namespace: django-api
data:
  # Django settings (non-sensitive)
  DJANGO_SETTINGS_MODULE: "config.settings.production"
  DEBUG: "False"
  ALLOWED_HOSTS: "api.example.com,*.api.example.com"
  
  # Database settings
  DB_HOST: "postgres-service"
  DB_PORT: "5432"
  DB_NAME: "django_db"
  
  # Redis settings
  REDIS_HOST: "redis-service"
  REDIS_PORT: "6379"
  
  # Application settings
  API_VERSION: "v1"
  LOG_LEVEL: "INFO"
  ENABLE_SWAGGER: "True"
```

**ConfigMap explained**: Stores non-sensitive configuration that might change between environments. Easy to update without rebuilding Docker images.

#### Step 3: Secrets

Create `k8s/secret.yml`:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: django-secrets
  namespace: django-api
type: Opaque
data:
  # All values must be base64 encoded
  # Encode: echo -n 'your-secret-value' | base64
  # Decode: echo 'base64-value' | base64 -d
  
  SECRET_KEY: eW91ci1zZWNyZXQta2V5LWhlcmU=  # your-secret-key-here
  DB_PASSWORD: ZGJwYXNzd29yZDEyMw==  # dbpassword123
  REDIS_PASSWORD: cmVkaXNwYXNzd29yZA==  # redispassword
  
  # API Keys
  STRIPE_SECRET_KEY: c2stdGVzdF8xMjM0NTY3ODkw  # sk-test_1234567890
  SENDGRID_API_KEY: U0cuQUJDREVGR0g=  # SG.ABCDEFGH
  AWS_SECRET_ACCESS_KEY: YXdzLXNlY3JldC1rZXk=  # aws-secret-key
```

**IMPORTANT**: Never commit real secrets to Git! Create secrets from command line or CI/CD:

```bash
# Create secret from literal values
kubectl create secret generic django-secrets \
  --from-literal=SECRET_KEY='your-actual-secret-key' \
  --from-literal=DB_PASSWORD='your-db-password' \
  --namespace=django-api

# Create secret from file
kubectl create secret generic django-secrets \
  --from-env-file=.env.production \
  --namespace=django-api
```

#### Step 4: PostgreSQL Deployment

Create `k8s/postgres.yml`:

```yaml
---
# PersistentVolumeClaim for database storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: django-api
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi  # 10GB storage for database
  storageClassName: standard  # Use default storage class

---
# PostgreSQL Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: django-api
spec:
  replicas: 1  # Database should have 1 replica
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          valueFrom:
            configMapKeyRef:
              name: django-config
              key: DB_NAME
        - name: POSTGRES_USER
          value: django_user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: django-secrets
              key: DB_PASSWORD
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc

---
# PostgreSQL Service
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: django-api
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP  # Internal only, not exposed to internet
```

**PostgreSQL Configuration Explained**:

- **PersistentVolumeClaim**: Requests storage that survives pod restarts (your data persists)
- **replicas: 1**: Databases need special handling for replication, start with one
- **env from secrets**: Passwords come from Secrets (encrypted at rest)
- **resources**: CPU/memory limits prevent database from consuming all node resources
- **ClusterIP**: Service is only accessible within cluster, not from internet (security)

#### Step 5: Redis Deployment

Create `k8s/redis.yml`:

```yaml
---
# Redis Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: django-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        command:
        - redis-server
        - --requirepass
        - $(REDIS_PASSWORD)
        env:
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: django-secrets
              key: REDIS_PASSWORD
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# Redis Service
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: django-api
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
  type: ClusterIP
```

#### Step 6: Django Application Deployment

Create `k8s/deployment.yml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: django-api
  namespace: django-api
  labels:
    app: django-api
spec:
  replicas: 3  # Run 3 instances for high availability
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Create 1 extra pod during update
      maxUnavailable: 1  # Max 1 pod can be unavailable
  selector:
    matchLabels:
      app: django-api
  template:
    metadata:
      labels:
        app: django-api
    spec:
      # Init container runs before main container
      initContainers:
      - name: migrate
        image: your-registry/django-api:latest
        command: ['python', 'manage.py', 'migrate', '--noinput']
        envFrom:
        - configMapRef:
            name: django-config
        - secretRef:
            name: django-secrets
      
      # Main application container
      containers:
      - name: django-api
        image: your-registry/django-api:latest
        ports:
        - containerPort: 8000
          name: http
        
        # Environment variables from ConfigMap and Secrets
        envFrom:
        - configMapRef:
            name: django-config
        - secretRef:
            name: django-secrets
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /health/
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health/ready/
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
        
        # Resource limits
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        
        # Mount volume for media files
        volumeMounts:
        - name: media-storage
          mountPath: /app/media
      
      volumes:
      - name: media-storage
        persistentVolumeClaim:
          claimName: media-pvc
```

**Deployment Configuration Explained**:

| Component | Purpose | Why It Matters |
|-----------|---------|----------------|
| **replicas: 3** | Run 3 copies of your app | If one crashes, others handle traffic (high availability) |
| **RollingUpdate** | Update pods one at a time | Zero-downtime deployments, always some pods running |
| **initContainers** | Run migrations before app starts | Database schema always matches code version |
| **livenessProbe** | Check if app is alive | Kubernetes restarts container if health check fails |
| **readinessProbe** | Check if app is ready for traffic | Don't send requests to pods still starting up |
| **resources** | CPU/memory limits | Prevent one pod from using all resources, enable autoscaling |

**Health Check Endpoints**: Create these in your Django app:

```python
# your_app/views.py
from django.http import JsonResponse
from django.db import connection

def health(request):
    """Liveness probe - is the app running?"""
    return JsonResponse({'status': 'healthy'})

def health_ready(request):
    """Readiness probe - is the app ready for traffic?"""
    try:
        # Check database connection
        connection.ensure_connection()
        return JsonResponse({'status': 'ready', 'database': 'connected'})
    except Exception as e:
        return JsonResponse({'status': 'not ready', 'error': str(e)}, status=503)
```

#### Step 7: Service

Create `k8s/service.yml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: django-api-service
  namespace: django-api
spec:
  selector:
    app: django-api
  ports:
  - protocol: TCP
    port: 80        # External port
    targetPort: 8000  # Container port
  type: LoadBalancer  # Create external load balancer
  sessionAffinity: ClientIP  # Sticky sessions (optional)
```

**Service Types**:

| Type | Use Case | Accessibility |
|------|----------|---------------|
| **ClusterIP** | Internal services (database, cache) | Only within cluster |
| **NodePort** | Development/testing | External via node IP:port |
| **LoadBalancer** | Production APIs | External via cloud load balancer |
| **Ingress** | Multiple services with routing | External via reverse proxy + SSL |

#### Step 8: Ingress (Advanced Routing)

Create `k8s/ingress.yml`:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: django-api-ingress
  namespace: django-api
  annotations:
    # Use nginx ingress controller
    kubernetes.io/ingress.class: "nginx"
    
    # Enable SSL redirect
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    
    # Enable CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "*"
    
    # Rate limiting
    nginx.ingress.kubernetes.io/limit-rps: "100"
    
    # SSL certificate from Let's Encrypt
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.example.com
    secretName: api-tls-secret
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: django-api-service
            port:
              number: 80
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: django-api-service
            port:
              number: 80
```

**Ingress Benefits**:

- **SSL Termination**: Handles HTTPS, issues free Let's Encrypt certificates
- **Path-based Routing**: Route `/api` to API service, `/admin` to admin service
- **Host-based Routing**: `api.example.com` ‚Üí API, `admin.example.com` ‚Üí Admin
- **Rate Limiting**: Built-in protection against abuse
- **Authentication**: Can add OAuth, Basic Auth at ingress level

#### Step 9: Horizontal Pod Autoscaler

Create `k8s/hpa.yml`:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: django-api-hpa
  namespace: django-api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: django-api
  minReplicas: 3    # Minimum pods
  maxReplicas: 10   # Maximum pods
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale when CPU > 70%
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Scale when memory > 80%
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60    # Wait 60s before scaling up
      policies:
      - type: Percent
        value: 50    # Scale up by 50% of current pods
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300   # Wait 5 min before scaling down
      policies:
      - type: Pods
        value: 1     # Scale down by 1 pod at a time
        periodSeconds: 60
```

**Autoscaling in Action**:

```
Time    | CPU | Replicas | Action
--------|-----|----------|----------------------------------
09:00   | 40% | 3        | Normal traffic, minimum replicas
10:00   | 75% | 3        | High CPU detected
10:01   | 75% | 5        | Scaled up by 50% (3 * 1.5 ‚âà 5)
10:05   | 80% | 5        | Still high, waiting
10:06   | 80% | 7        | Scaled up again
11:00   | 45% | 7        | Traffic decreasing
11:05   | 45% | 6        | Scaled down by 1 (gradual)
12:00   | 30% | 3        | Back to minimum
```

### 3.5 Deploying to Kubernetes

#### Apply all configurations:

```bash
# Create namespace
kubectl apply -f k8s/namespace.yml

# Create ConfigMap and Secrets
kubectl apply -f k8s/configmap.yml
kubectl apply -f k8s/secret.yml

# Deploy database and cache
kubectl apply -f k8s/postgres.yml
kubectl apply -f k8s/redis.yml

# Wait for database to be ready
kubectl wait --for=condition=available --timeout=300s deployment/postgres -n django-api

# Deploy Django application
kubectl apply -f k8s/deployment.yml
kubectl apply -f k8s/service.yml
kubectl apply -f k8s/ingress.yml
kubectl apply -f k8s/hpa.yml

# Check deployment status
kubectl get all -n django-api

# View pod logs
kubectl logs -f deployment/django-api -n django-api

# Check autoscaler status
kubectl get hpa -n django-api
```

#### Useful Kubernetes Commands:

```bash
# Get all resources in namespace
kubectl get all -n django-api

# Describe deployment (detailed info)
kubectl describe deployment django-api -n django-api

# View pod logs
kubectl logs <pod-name> -n django-api

# Follow logs (like tail -f)
kubectl logs -f <pod-name> -n django-api

# Execute command in pod (like docker exec)
kubectl exec -it <pod-name> -n django-api -- /bin/bash

# Port forward for local access
kubectl port-forward service/django-api-service 8000:80 -n django-api

# Scale manually
kubectl scale deployment django-api --replicas=5 -n django-api

# Rolling restart
kubectl rollout restart deployment/django-api -n django-api

# Rollback to previous version
kubectl rollout undo deployment/django-api -n django-api

# Check rollout status
kubectl rollout status deployment/django-api -n django-api

# Delete all resources
kubectl delete namespace django-api
```

### 3.6 Monitoring Kubernetes

Install Prometheus and Grafana for monitoring:

```bash
# Add Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Install Prometheus + Grafana
helm install monitoring prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace

# Access Grafana dashboard
kubectl port-forward svc/monitoring-grafana 3000:80 -n monitoring
# Open http://localhost:3000 (default credentials: admin/prom-operator)
```

**What You Can Monitor**:

- **Pod metrics**: CPU, memory, network, disk usage per pod
- **Node metrics**: Resource usage across entire cluster
- **API metrics**: Request rate, latency, error rate
- **Database metrics**: Connections, query performance
- **Autoscaling**: Current replicas, scaling events
- **Custom metrics**: Your application-specific metrics

---

*This is part 1 of guide002.md. The content continues with sections on API Gateway, Machine Learning, Elasticsearch, and Analytics. Due to length constraints, I'm creating this in parts.*

## 4. API Gateway Integration

### 4.1 What is an API Gateway

An **API Gateway** is a server that acts as a single entry point for all client requests to your microservices. It handles routing, authentication, rate limiting, caching, and more.

#### Why Use an API Gateway

**Without API Gateway**:
```
Client ‚Üí Service A (auth, rate limit, logging)
Client ‚Üí Service B (auth, rate limit, logging)
Client ‚Üí Service C (auth, rate limit, logging)
Problem: Duplicate logic in every service
```

**With API Gateway**:
```
Client ‚Üí API Gateway (auth, rate limit, logging, routing)
           ‚îú‚Üí Service A (business logic only)
           ‚îú‚Üí Service B (business logic only)
           ‚îî‚Üí Service C (business logic only)
Benefit: Centralized cross-cutting concerns
```

**Key Benefits**:

| Benefit | Explanation | Real-World Impact |
|---------|-------------|-------------------|
| **Single Entry Point** | All APIs accessed through one URL | Easier client integration, simpler DNS setup |
| **Authentication** | Centralized auth logic | Write auth once, not per service |
| **Rate Limiting** | Protect all services at once | Prevent abuse across entire API |
| **Caching** | Cache responses at gateway | Reduce backend load, faster responses |
| **SSL Termination** | Handle HTTPS at gateway | Backend services don't need SSL certificates |
| **Request Transformation** | Modify requests/responses | Maintain backward compatibility during migration |
| **Analytics** | Centralized logging and metrics | Single dashboard for all API traffic |

### 4.2 Kong API Gateway

Kong is the most popular open-source API Gateway, built on nginx for high performance.

#### Step 1: Install Kong with Docker Compose

Create `docker-compose-kong.yml`:

```yaml
version: '3.8'

services:
  # Kong Database
  kong-database:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: kong
      POSTGRES_DB: kong
      POSTGRES_PASSWORD: kong_password
    volumes:
      - kong-db-data:/var/lib/postgresql/data
    networks:
      - kong-net
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "kong"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kong Migration (run once to setup database)
  kong-migration:
    image: kong:3.4
    command: kong migrations bootstrap
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_DATABASE: kong
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kong_password
    depends_on:
      kong-database:
        condition: service_healthy
    networks:
      - kong-net
    restart: on-failure

  # Kong Gateway
  kong:
    image: kong:3.4
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_DATABASE: kong
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kong_password
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: "0.0.0.0:8001"
      KONG_ADMIN_GUI_URL: "http://localhost:8002"
    depends_on:
      kong-database:
        condition: service_healthy
      kong-migration:
        condition: service_completed_successfully
    ports:
      - "8000:8000"  # Proxy HTTP
      - "8443:8443"  # Proxy HTTPS
      - "8001:8001"  # Admin API HTTP
      - "8444:8444"  # Admin API HTTPS
      - "8002:8002"  # Kong Manager (GUI)
    networks:
      - kong-net
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Konga (Kong Admin UI)
  konga:
    image: pantsel/konga:latest
    environment:
      NODE_ENV: production
      DB_ADAPTER: postgres
      DB_URI: postgresql://kong:kong_password@kong-database:5432/konga
    depends_on:
      kong-database:
        condition: service_healthy
    ports:
      - "1337:1337"
    networks:
      - kong-net

volumes:
  kong-db-data:

networks:
  kong-net:
    driver: bridge
```

Start Kong:

```bash
docker-compose -f docker-compose-kong.yml up -d

# Wait for Kong to be ready
curl -i http://localhost:8001/

# Access Konga UI
# Open http://localhost:1337
# Create admin account on first visit
```

#### Step 2: Configure Services and Routes

**Register Your Django API as a Service**:

```bash
# Create a service
curl -i -X POST http://localhost:8001/services/ \
  --data name=django-api \
  --data url='http://host.docker.internal:8000'

# Create a route for the service
curl -i -X POST http://localhost:8001/services/django-api/routes \
  --data 'paths[]=/api' \
  --data name=django-api-route
```

Now you can access your Django API through Kong:
```bash
# Before: Direct access
curl http://localhost:8000/api/books/

# After: Through Kong (same result, but with gateway features)
curl http://localhost:8000/api/books/
```

#### Step 3: Add Authentication

**Enable JWT Plugin**:

```bash
# Enable JWT authentication on the service
curl -i -X POST http://localhost:8001/services/django-api/plugins \
  --data name=jwt \
  --data config.secret_is_base64=false

# Create a consumer (API user)
curl -i -X POST http://localhost:8001/consumers/ \
  --data username=mobile-app

# Create JWT credentials for the consumer
curl -i -X POST http://localhost:8001/consumers/mobile-app/jwt \
  --data algorithm=HS256 \
  --data secret=your-secret-key \
  --data key=mobile-app-key
```

Now requests need JWT token:

```python
# Python client example
import jwt
import requests
import time

# Generate JWT token
payload = {
    'iss': 'mobile-app-key',  # Issuer (key from Kong)
    'exp': time.time() + 3600  # Expiration (1 hour)
}
token = jwt.encode(payload, 'your-secret-key', algorithm='HS256')

# Make authenticated request
headers = {'Authorization': f'Bearer {token}'}
response = requests.get('http://localhost:8000/api/books/', headers=headers)
print(response.json())
```

#### Step 4: Add Rate Limiting

```bash
# Limit to 100 requests per minute
curl -i -X POST http://localhost:8001/services/django-api/plugins \
  --data name=rate-limiting \
  --data config.minute=100 \
  --data config.hour=1000 \
  --data config.policy=local

# Different limits for different consumers
curl -i -X POST http://localhost:8001/consumers/mobile-app/plugins \
  --data name=rate-limiting \
  --data config.minute=200 \
  --data config.hour=5000
```

**Rate Limiting in Action**:

```bash
# Make 101 requests rapidly
for i in {1..101}; do
  curl http://localhost:8000/api/books/
done

# Request 101 will return:
# HTTP/1.1 429 Too Many Requests
# {
#   "message": "API rate limit exceeded"
# }
```

#### Step 5: Add Caching

```bash
# Enable proxy caching
curl -i -X POST http://localhost:8001/services/django-api/plugins \
  --data name=proxy-cache \
  --data config.strategy=memory \
  --data config.content_type="application/json" \
  --data config.cache_ttl=300 \
  --data config.cache_control=true

# Cache GET requests for 5 minutes
# Subsequent identical requests served from cache (much faster)
```

#### Step 6: Add Request Transformation

```bash
# Add custom header to all requests
curl -i -X POST http://localhost:8001/services/django-api/plugins \
  --data name=request-transformer \
  --data config.add.headers=X-Gateway:Kong \
  --data config.add.headers=X-Request-ID:\$request_id

# Remove sensitive headers
curl -i -X POST http://localhost:8001/services/django-api/plugins \
  --data name=response-transformer \
  --data config.remove.headers=X-Django-Version
```

#### Step 7: Add CORS

```bash
# Enable CORS for frontend applications
curl -i -X POST http://localhost:8001/services/django-api/plugins \
  --data name=cors \
  --data config.origins=http://localhost:3000 \
  --data config.origins=https://app.example.com \
  --data config.methods=GET \
  --data config.methods=POST \
  --data config.methods=PUT \
  --data config.methods=DELETE \
  --data config.headers=Authorization \
  --data config.headers=Content-Type \
  --data config.exposed_headers=X-Total-Count \
  --data config.credentials=true \
  --data config.max_age=3600
```

### 4.3 AWS API Gateway

AWS API Gateway is a fully managed service for creating and managing APIs at scale.

#### Step 1: Create REST API

```bash
# Install AWS CLI
pip install awscli

# Configure AWS credentials
aws configure

# Create REST API
aws apigateway create-rest-api \
  --name 'Django API' \
  --description 'Django REST Framework API' \
  --endpoint-configuration types=REGIONAL
```

#### Step 2: Create Resource and Method

```bash
# Get API ID from previous command
API_ID=your-api-id

# Get root resource ID
ROOT_ID=$(aws apigateway get-resources \
  --rest-api-id $API_ID \
  --query 'items[0].id' \
  --output text)

# Create /books resource
BOOKS_ID=$(aws apigateway create-resource \
  --rest-api-id $API_ID \
  --parent-id $ROOT_ID \
  --path-part books \
  --query 'id' \
  --output text)

# Create GET method on /books
aws apigateway put-method \
  --rest-api-id $API_ID \
  --resource-id $BOOKS_ID \
  --http-method GET \
  --authorization-type NONE \
  --api-key-required
```

#### Step 3: Create Integration

```bash
# Connect to your Django backend
aws apigateway put-integration \
  --rest-api-id $API_ID \
  --resource-id $BOOKS_ID \
  --http-method GET \
  --type HTTP_PROXY \
  --integration-http-method GET \
  --uri 'http://your-django-server.com/api/books/'

# Configure integration response
aws apigateway put-integration-response \
  --rest-api-id $API_ID \
  --resource-id $BOOKS_ID \
  --http-method GET \
  --status-code 200 \
  --selection-pattern ''

# Configure method response
aws apigateway put-method-response \
  --rest-api-id $API_ID \
  --resource-id $BOOKS_ID \
  --http-method GET \
  --status-code 200 \
  --response-models '{"application/json": "Empty"}'
```

#### Step 4: Deploy API

```bash
# Create deployment
aws apigateway create-deployment \
  --rest-api-id $API_ID \
  --stage-name prod

# Get invoke URL
echo "https://$API_ID.execute-api.us-east-1.amazonaws.com/prod/books"
```

#### Step 5: Add API Key and Usage Plan

```bash
# Create API key
KEY_ID=$(aws apigateway create-api-key \
  --name 'Mobile App Key' \
  --enabled \
  --query 'id' \
  --output text)

# Get API key value
aws apigateway get-api-key \
  --api-key $KEY_ID \
  --include-value

# Create usage plan (100k requests/month)
PLAN_ID=$(aws apigateway create-usage-plan \
  --name 'Basic Plan' \
  --throttle burstLimit=100,rateLimit=50 \
  --quota limit=100000,period=MONTH \
  --query 'id' \
  --output text)

# Associate API with usage plan
aws apigateway create-usage-plan-key \
  --usage-plan-id $PLAN_ID \
  --key-id $KEY_ID \
  --key-type API_KEY
```

#### Step 6: Test API Gateway

```bash
# Get your API key
API_KEY=your-api-key-value

# Make request with API key
curl -H "x-api-key: $API_KEY" \
  https://$API_ID.execute-api.us-east-1.amazonaws.com/prod/books
```

### 4.4 API Gateway Best Practices

#### DO's ‚úÖ

| Practice | Benefit | Implementation |
|----------|---------|----------------|
| **Use API versioning** | Maintain backward compatibility | `/v1/books`, `/v2/books` |
| **Implement rate limiting** | Prevent abuse | Different limits per tier (free, basic, premium) |
| **Enable caching** | Reduce backend load | Cache GET requests, invalidate on POST/PUT/DELETE |
| **Monitor metrics** | Track performance | Request count, latency, error rate per endpoint |
| **Use request validation** | Reject bad requests early | Validate headers, query params, request body |
| **Implement circuit breaker** | Protect failing services | Stop sending requests to unhealthy backends |

#### DON'Ts ‚ùå

| Anti-pattern | Problem | Better Approach |
|--------------|---------|-----------------|
| **Expose internal errors** | Security risk | Return generic errors through gateway |
| **Skip authentication** | Unauthorized access | Always authenticate at gateway level |
| **Large request/response** | Slow performance | Limit payload size, use pagination |
| **No timeout configuration** | Hanging requests | Set timeouts (e.g., 30 seconds max) |
| **Ignore CORS** | Frontend can't access API | Configure CORS properly |

---

## 5. Machine Learning Integration

### 5.1 Why Add Machine Learning to Your API

Machine Learning can transform your API from passive data provider to intelligent assistant:

**Book Recommendation Engine Example**:

| Without ML | With ML | Impact |
|------------|---------|--------|
| Show random books | Show books user will love | **Higher conversion** |
| Generic homepage | Personalized for each user | **Better engagement** |
| Search exact matches | Search understands intent | **Better user experience** |
| Manual categorization | Auto-categorize new books | **Reduced manual work** |

### 5.2 Building a Recommendation Engine

We'll build a collaborative filtering recommendation engine that suggests books based on user behavior.

#### Step 1: Install ML Libraries

```bash
pip install scikit-learn pandas numpy joblib
```

Update `requirements.txt`:

```
scikit-learn==1.3.0
pandas==2.0.0
numpy==1.24.0
joblib==1.3.0
```

#### Step 2: Create ML Models

Create `books/ml/models.py`:

```python
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from django.core.cache import cache
import joblib
import os


class BookRecommender:
    """
    Collaborative filtering + content-based recommendation engine.
    
    How it works:
    1. User-based: Recommend books liked by similar users
    2. Item-based: Recommend books similar to what user liked
    3. Content-based: Recommend books with similar descriptions/categories
    """
    
    def __init__(self):
        self.user_similarity_matrix = None
        self.item_similarity_matrix = None
        self.content_vectorizer = None
        self.content_features = None
        self.model_path = 'ml_models/'
        os.makedirs(self.model_path, exist_ok=True)
    
    def train(self, ratings_df, books_df):
        """
        Train the recommendation model.
        
        Args:
            ratings_df: DataFrame with columns [user_id, book_id, rating]
            books_df: DataFrame with columns [book_id, title, description, categories]
        """
        print("Training recommendation model...")
        
        # 1. Create user-item matrix
        user_item_matrix = ratings_df.pivot_table(
            index='user_id',
            columns='book_id',
            values='rating',
            fill_value=0
        )
        
        # 2. Calculate user similarity (users who rate books similarly)
        self.user_similarity_matrix = cosine_similarity(user_item_matrix)
        
        # 3. Calculate item similarity (books rated similarly)
        self.item_similarity_matrix = cosine_similarity(user_item_matrix.T)
        
        # 4. Content-based features (book descriptions and categories)
        books_df['content'] = (
            books_df['title'] + ' ' + 
            books_df['description'] + ' ' + 
            books_df['categories']
        )
        
        self.content_vectorizer = TfidfVectorizer(
            max_features=500,
            stop_words='english'
        )
        self.content_features = self.content_vectorizer.fit_transform(
            books_df['content']
        )
        
        # 5. Save models
        self._save_model()
        print("Model training complete!")
    
    def recommend_for_user(self, user_id, n_recommendations=10):
        """
        Recommend books for a specific user.
        
        Strategy:
        1. Find similar users
        2. Get books they liked
        3. Filter out books user already rated
        4. Rank by predicted rating
        """
        # Check cache first
        cache_key = f'recommendations_{user_id}'
        cached = cache.get(cache_key)
        if cached:
            return cached
        
        # Get user's ratings
        user_ratings = self._get_user_ratings(user_id)
        
        # Find similar users
        similar_users = self._find_similar_users(user_id, top_n=20)
        
        # Aggregate recommendations
        recommendations = {}
        for similar_user_id, similarity in similar_users:
            similar_user_ratings = self._get_user_ratings(similar_user_id)
            
            for book_id, rating in similar_user_ratings.items():
                if book_id not in user_ratings:
                    if book_id not in recommendations:
                        recommendations[book_id] = 0
                    recommendations[book_id] += rating * similarity
        
        # Sort by score
        top_books = sorted(
            recommendations.items(),
            key=lambda x: x[1],
            reverse=True
        )[:n_recommendations]
        
        result = [book_id for book_id, _ in top_books]
        
        # Cache for 1 hour
        cache.set(cache_key, result, 3600)
        
        return result
    
    def recommend_similar_books(self, book_id, n_recommendations=10):
        """
        Find books similar to a given book.
        
        Uses both:
        1. User rating patterns (collaborative filtering)
        2. Content similarity (descriptions, categories)
        """
        cache_key = f'similar_books_{book_id}'
        cached = cache.get(cache_key)
        if cached:
            return cached
        
        # Get content similarity
        book_idx = self._get_book_index(book_id)
        content_scores = cosine_similarity(
            self.content_features[book_idx],
            self.content_features
        ).flatten()
        
        # Get collaborative filtering similarity
        cf_scores = self.item_similarity_matrix[book_idx]
        
        # Combine both (60% collaborative, 40% content)
        combined_scores = 0.6 * cf_scores + 0.4 * content_scores
        
        # Get top similar books
        similar_indices = combined_scores.argsort()[::-1][1:n_recommendations+1]
        result = [self._get_book_id(idx) for idx in similar_indices]
        
        cache.set(cache_key, result, 3600)
        return result
    
    def _find_similar_users(self, user_id, top_n=20):
        """Find users with similar rating patterns."""
        user_idx = self._get_user_index(user_id)
        similarities = self.user_similarity_matrix[user_idx]
        similar_user_indices = similarities.argsort()[::-1][1:top_n+1]
        
        return [
            (self._get_user_id(idx), similarities[idx])
            for idx in similar_user_indices
        ]
    
    def _save_model(self):
        """Save trained model to disk."""
        joblib.dump(self.user_similarity_matrix, 
                    f'{self.model_path}/user_similarity.pkl')
        joblib.dump(self.item_similarity_matrix,
                    f'{self.model_path}/item_similarity.pkl')
        joblib.dump(self.content_vectorizer,
                    f'{self.model_path}/content_vectorizer.pkl')
        joblib.dump(self.content_features,
                    f'{self.model_path}/content_features.pkl')
    
    def load_model(self):
        """Load pre-trained model from disk."""
        try:
            self.user_similarity_matrix = joblib.load(
                f'{self.model_path}/user_similarity.pkl'
            )
            self.item_similarity_matrix = joblib.load(
                f'{self.model_path}/item_similarity.pkl'
            )
            self.content_vectorizer = joblib.load(
                f'{self.model_path}/content_vectorizer.pkl'
            )
            self.content_features = joblib.load(
                f'{self.model_path}/content_features.pkl'
            )
            return True
        except FileNotFoundError:
            return False
    
    # Helper methods (simplified for brevity)
    def _get_user_ratings(self, user_id):
        """Get all ratings by a user."""
        from books.models import Review
        return dict(
            Review.objects.filter(user_id=user_id).values_list('book_id', 'rating')
        )
    
    def _get_user_index(self, user_id):
        """Map user_id to matrix index."""
        # Implementation depends on your data structure
        pass
    
    def _get_user_id(self, index):
        """Map matrix index to user_id."""
        pass
    
    def _get_book_index(self, book_id):
        """Map book_id to matrix index."""
        pass
    
    def _get_book_id(self, index):
        """Map matrix index to book_id."""
        pass


class SearchRanker:
    """
    ML-based search ranking.
    
    Ranks search results based on:
    1. Text relevance (TF-IDF)
    2. Popularity (reviews, ratings)
    3. User preferences (personalization)
    """
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.is_trained = False
    
    def train(self, books_df):
        """Train ranking model on book data."""
        # Create text features
        books_df['search_text'] = (
            books_df['title'] + ' ' * 3 +  # Title weighted 3x
            books_df['author'] + ' ' * 2 +  # Author weighted 2x
            books_df['description'] + ' ' +
            books_df['categories']
        )
        
        self.vectorizer.fit(books_df['search_text'])
        self.is_trained = True
    
    def rank_results(self, query, book_ids, user_id=None):
        """
        Rank search results.
        
        Returns book IDs sorted by relevance.
        """
        from books.models import Book, Review
        
        # Get books
        books = Book.objects.filter(id__in=book_ids).select_related('author')
        
        # Calculate scores
        scores = []
        for book in books:
            # Text relevance score
            text_score = self._calculate_text_relevance(query, book)
            
            # Popularity score
            popularity_score = self._calculate_popularity(book)
            
            # Personalization score
            personalization_score = 0
            if user_id:
                personalization_score = self._calculate_personalization(
                    user_id, book
                )
            
            # Combined score (weighted)
            total_score = (
                0.5 * text_score +
                0.3 * popularity_score +
                0.2 * personalization_score
            )
            
            scores.append((book.id, total_score))
        
        # Sort by score
        ranked_ids = [
            book_id for book_id, _ in 
            sorted(scores, key=lambda x: x[1], reverse=True)
        ]
        
        return ranked_ids
    
    def _calculate_text_relevance(self, query, book):
        """TF-IDF based relevance."""
        query_vec = self.vectorizer.transform([query])
        book_text = f"{book.title} {book.description}"
        book_vec = self.vectorizer.transform([book_text])
        
        similarity = cosine_similarity(query_vec, book_vec)[0][0]
        return similarity
    
    def _calculate_popularity(self, book):
        """Popularity based on reviews and ratings."""
        review_count = book.reviews.count()
        avg_rating = book.reviews.aggregate(Avg('rating'))['rating__avg'] or 0
        
        # Weighted score
        return (review_count * 0.3 + avg_rating * 0.7) / 5.0
    
    def _calculate_personalization(self, user_id, book):
        """Personalization based on user history."""
        from books.models import Review
        
        # Check if user liked similar books
        user_liked_categories = Review.objects.filter(
            user_id=user_id,
            rating__gte=4
        ).values_list('book__categories', flat=True)
        
        # Calculate category overlap
        book_categories = set(book.categories.values_list('id', flat=True))
        user_categories = set(user_liked_categories)
        
        if not user_categories:
            return 0.5  # Neutral score
        
        overlap = len(book_categories & user_categories)
        return overlap / len(user_categories)
```

**Understanding the Recommendation System**:

| Component | What It Does | Simple Explanation |
|-----------|--------------|-------------------|
| **Collaborative Filtering** | Find users with similar tastes | "Users who rated like you also loved these books" |
| **Content-Based** | Find books with similar content | "Books about dragons if you liked 'Game of Thrones'" |
| **TF-IDF Vectorizer** | Convert text to numbers | Measures word importance (e.g., "dragon" is important in fantasy, not "the") |
| **Cosine Similarity** | Measure how similar two items are | 1.0 = identical, 0.0 = completely different |
| **Hybrid Approach** | Combine multiple techniques | Get best of both worlds, better recommendations |

#### Step 3: Training Management Command

Create `books/management/commands/train_ml_models.py`:

```python
from django.core.management.base import BaseCommand
from books.ml.models import BookRecommender, SearchRanker
from books.models import Review, Book
import pandas as pd


class Command(BaseCommand):
    help = 'Train ML recommendation models'
    
    def handle(self, *args, **options):
        self.stdout.write('Starting ML model training...')
        
        # Prepare training data
        self.stdout.write('Preparing data...')
        
        # Get ratings data
        ratings = Review.objects.values('user_id', 'book_id', 'rating')
        ratings_df = pd.DataFrame(list(ratings))
        
        # Get books data
        books = Book.objects.select_related('author').prefetch_related('categories')
        books_data = []
        for book in books:
            books_data.append({
                'book_id': book.id,
                'title': book.title,
                'description': book.description,
                'categories': ' '.join(book.categories.values_list('name', flat=True))
            })
        books_df = pd.DataFrame(books_data)
        
        # Train recommender
        self.stdout.write('Training recommendation engine...')
        recommender = BookRecommender()
        recommender.train(ratings_df, books_df)
        
        # Train search ranker
        self.stdout.write('Training search ranker...')
        ranker = SearchRanker()
        ranker.train(books_df)
        
        self.stdout.write(self.style.SUCCESS('ML models trained successfully!'))
```

Run training:

```bash
python manage.py train_ml_models
```

#### Step 4: API Endpoints

Add recommendation endpoints to `books/views.py`:

```python
from rest_framework.decorators import action
from rest_framework.response import Response
from .ml.models import BookRecommender


class BookViewSet(viewsets.ModelViewSet):
    # ... existing code ...
    
    @action(detail=False, methods=['get'])
    def recommended(self, request):
        """
        Get personalized book recommendations for current user.
        
        GET /api/books/recommended/
        """
        if not request.user.is_authenticated:
            return Response(
                {'error': 'Authentication required for personalized recommendations'},
                status=status.HTTP_401_UNAUTHORIZED
            )
        
        # Load recommender
        recommender = BookRecommender()
        if not recommender.load_model():
            return Response(
                {'error': 'Recommendation model not trained yet'},
                status=status.HTTP_503_SERVICE_UNAVAILABLE
            )
        
        # Get recommendations
        recommended_ids = recommender.recommend_for_user(
            request.user.id,
            n_recommendations=20
        )
        
        # Get book objects
        books = Book.objects.filter(id__in=recommended_ids)
        
        # Maintain order from ML model
        books_dict = {book.id: book for book in books}
        ordered_books = [books_dict[book_id] for book_id in recommended_ids if book_id in books_dict]
        
        serializer = self.get_serializer(ordered_books, many=True)
        return Response(serializer.data)
    
    @action(detail=True, methods=['get'])
    def similar(self, request, pk=None):
        """
        Get books similar to this book.
        
        GET /api/books/{id}/similar/
        """
        book = self.get_object()
        
        recommender = BookRecommender()
        if not recommender.load_model():
            return Response(
                {'error': 'Recommendation model not trained yet'},
                status=status.HTTP_503_SERVICE_UNAVAILABLE
            )
        
        similar_ids = recommender.recommend_similar_books(
            book.id,
            n_recommendations=10
        )
        
        similar_books = Book.objects.filter(id__in=similar_ids)
        books_dict = {book.id: book for book in similar_books}
        ordered_books = [books_dict[book_id] for book_id in similar_ids if book_id in books_dict]
        
        serializer = self.get_serializer(ordered_books, many=True)
        return Response(serializer.data)
```

#### Step 5: Celery Task for Periodic Retraining

Create `books/tasks.py`:

```python
from celery import shared_task
from django.core.management import call_command


@shared_task
def retrain_ml_models():
    """
    Retrain ML models with latest data.
    Schedule this to run daily.
    """
    call_command('train_ml_models')
    return 'ML models retrained successfully'
```

Configure Celery beat schedule in `config/settings.py`:

```python
from celery.schedules import crontab

CELERY_BEAT_SCHEDULE = {
    'retrain-ml-models': {
        'task': 'books.tasks.retrain_ml_models',
        'schedule': crontab(hour=2, minute=0),  # Run at 2 AM daily
    },
}
```

### 5.3 ML Best Practices

#### DO's ‚úÖ

| Practice | Why | How |
|----------|-----|-----|
| **Start simple** | Complex models are hard to maintain | Begin with basic collaborative filtering, add complexity if needed |
| **Cache predictions** | ML inference is expensive | Cache recommendations for 1-24 hours depending on freshness needs |
| **Retrain regularly** | User preferences change | Retrain daily or weekly with new data |
| **A/B test** | Measure real impact | Show ML recommendations to 50% of users, random to 50%, compare |
| **Monitor performance** | Catch degradation early | Track click-through rate, conversion rate on recommendations |
| **Use async processing** | Don't block API requests | Generate recommendations in background Celery task |

#### DON'Ts ‚ùå

| Anti-pattern | Problem | Solution |
|--------------|---------|----------|
| **Synchronous training in API** | Request timeout, poor UX | Train via management command or Celery task |
| **No fallback** | Broken experience for new users | Show popular items if ML unavailable |
| **Ignore cold start** | Bad recommendations for new users/items | Use content-based filtering for new users, popularity for new items |
| **Over-personalization** | Filter bubble, missed discoveries | Mix personalized (70%) with popular/trending (30%) |
| **No explanation** | Users don't trust recommendations | Add "Because you liked X" or "Trending in category Y" |

---

*Content continues in next part due to length. The guide will continue with Elasticsearch, Analytics, and Best Practices sections...*

## 6. Elasticsearch Search Implementation

### 6.1 Why Elasticsearch

**Problem with Database Search**:
```sql
SELECT * FROM books WHERE title LIKE '%django%' OR description LIKE '%django%';
-- Problems:
-- ‚ùå Slow on large datasets (full table scan)
-- ‚ùå No relevance ranking
-- ‚ùå No fuzzy matching (typos)
-- ‚ùå No faceted search
-- ‚ùå Poor for complex queries
```

**Elasticsearch Solution**:
```
GET /books/_search
{
  "query": {
    "multi_match": {
      "query": "djngo rest",  # Typo tolerated!
      "fields": ["title^3", "description", "author"],
      "fuzziness": "AUTO"
    }
  }
}
-- Benefits:
-- ‚úÖ Fast full-text search (milliseconds)
-- ‚úÖ Relevance scoring
-- ‚úÖ Fuzzy matching (handles typos)
-- ‚úÖ Faceted search (filter by category, price, rating)
-- ‚úÖ Aggregations (analytics)
```

### 6.2 Setting Up Elasticsearch

#### Step 1: Install Elasticsearch with Docker

Create `docker-compose-elasticsearch.yml`:

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  # Memory settings
      - xpack.security.enabled=false  # Disable security for development
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - elastic

  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - elastic

volumes:
  elasticsearch-data:

networks:
  elastic:
    driver: bridge
```

Start Elasticsearch:

```bash
docker-compose -f docker-compose-elasticsearch.yml up -d

# Verify Elasticsearch is running
curl http://localhost:9200/

# Access Kibana UI (for debugging queries)
# Open http://localhost:5601
```

#### Step 2: Install Python Elasticsearch Client

```bash
pip install elasticsearch-dsl django-elasticsearch-dsl
```

Update `requirements.txt`:

```
elasticsearch==8.10.0
elasticsearch-dsl==8.9.0
django-elasticsearch-dsl==8.0.0
```

#### Step 3: Configure Django

Add to `config/settings.py`:

```python
INSTALLED_APPS = [
    # ... existing apps
    'django_elasticsearch_dsl',
]

# Elasticsearch configuration
ELASTICSEARCH_DSL = {
    'default': {
        'hosts': 'localhost:9200'
    },
}

# Elasticsearch index settings
ELASTICSEARCH_INDEX_SETTINGS = {
    'number_of_shards': 1,
    'number_of_replicas': 0,  # 0 for development, 1+ for production
    'analysis': {
        'analyzer': {
            'book_analyzer': {
                'type': 'custom',
                'tokenizer': 'standard',
                'filter': [
                    'lowercase',
                    'stop',
                    'snowball',
                    'english_possessive_stemmer'
                ]
            }
        },
        'filter': {
            'english_possessive_stemmer': {
                'type': 'stemmer',
                'language': 'possessive_english'
            },
            'snowball': {
                'type': 'snowball',
                'language': 'English'
            }
        }
    }
}
```

**What this configuration does**:

| Setting | Purpose | Example |
|---------|---------|---------|
| **custom analyzer** | How to process text | Converts "BOOKS" ‚Üí "book", removes "the", "a" |
| **snowball filter** | Stemming (reduce words to root) | "running" ‚Üí "run", "books" ‚Üí "book" |
| **lowercase filter** | Case-insensitive search | "Django" matches "django" |
| **stop filter** | Remove common words | Ignores "the", "is", "at" to improve relevance |

#### Step 4: Create Elasticsearch Document

Create `books/documents.py`:

```python
from django_elasticsearch_dsl import Document, fields
from django_elasticsearch_dsl.registries import registry
from .models import Book, Author, Category


@registry.register_document
class BookDocument(Document):
    """
    Elasticsearch document for Book model.
    
    Maps Django model fields to Elasticsearch fields with appropriate types and analyzers.
    """
    
    # Author information (nested object)
    author = fields.ObjectField(properties={
        'id': fields.IntegerField(),
        'first_name': fields.TextField(
            analyzer='book_analyzer',
            fields={'raw': fields.KeywordField()}  # For exact match and sorting
        ),
        'last_name': fields.TextField(
            analyzer='book_analyzer',
            fields={'raw': fields.KeywordField()}
        ),
        'full_name': fields.TextField(analyzer='book_analyzer'),
    })
    
    # Categories (nested array)
    categories = fields.NestedField(properties={
        'id': fields.IntegerField(),
        'name': fields.TextField(
            analyzer='book_analyzer',
            fields={'raw': fields.KeywordField()}
        ),
    })
    
    # Publisher information
    publisher = fields.ObjectField(properties={
        'id': fields.IntegerField(),
        'name': fields.TextField(analyzer='book_analyzer'),
    })
    
    # Computed fields
    average_rating = fields.FloatField()
    review_count = fields.IntegerField()
    
    # Text fields with different boost values for relevance
    title = fields.TextField(
        analyzer='book_analyzer',
        fields={
            'raw': fields.KeywordField(),
            'suggest': fields.CompletionField()  # For autocomplete
        },
        boost=3.0  # Title matches are 3x more important
    )
    
    description = fields.TextField(
        analyzer='book_analyzer',
        boost=1.0  # Normal importance
    )
    
    # Facets for filtering
    status = fields.KeywordField()
    price = fields.FloatField()
    publication_date = fields.DateField()
    
    class Index:
        name = 'books'  # Elasticsearch index name
        settings = {
            'number_of_shards': 1,
            'number_of_replicas': 0,
            'refresh_interval': '5s',  # How often to refresh index
        }
    
    class Django:
        model = Book
        fields = [
            'id',
            'isbn',
            'pages',
            'created_at',
            'updated_at',
        ]
        related_models = [Author, Category]  # Auto-update when related models change
    
    def get_queryset(self):
        """Optimize database queries for indexing."""
        return super().get_queryset().select_related(
            'author',
            'publisher'
        ).prefetch_related(
            'categories',
            'reviews'
        )
    
    def get_instances_from_related(self, related_instance):
        """
        If related model is updated, update book documents.
        Example: If author name changes, re-index all their books.
        """
        if isinstance(related_instance, Author):
            return related_instance.books.all()
        elif isinstance(related_instance, Category):
            return related_instance.books.all()
    
    def prepare_author(self, instance):
        """Prepare author data for indexing."""
        if instance.author:
            return {
                'id': instance.author.id,
                'first_name': instance.author.first_name,
                'last_name': instance.author.last_name,
                'full_name': instance.author.full_name,
            }
        return None
    
    def prepare_categories(self, instance):
        """Prepare categories data for indexing."""
        return [
            {
                'id': category.id,
                'name': category.name,
            }
            for category in instance.categories.all()
        ]
    
    def prepare_publisher(self, instance):
        """Prepare publisher data for indexing."""
        if instance.publisher:
            return {
                'id': instance.publisher.id,
                'name': instance.publisher.name,
            }
        return None
    
    def prepare_average_rating(self, instance):
        """Calculate average rating."""
        from django.db.models import Avg
        avg = instance.reviews.aggregate(Avg('rating'))['rating__avg']
        return round(avg, 2) if avg else 0
    
    def prepare_review_count(self, instance):
        """Count reviews."""
        return instance.reviews.count()
```

**Document Structure Explained**:

| Field Type | Use Case | Example |
|------------|----------|---------|
| **TextField** | Full-text search | Book titles, descriptions (analyzed for search) |
| **KeywordField** | Exact match, filtering, sorting | Status ("available"), ISBN |
| **IntegerField** | Numeric filtering, sorting | Price range, page count |
| **DateField** | Date filtering, sorting | Publication date |
| **ObjectField** | Nested objects (single) | Author information |
| **NestedField** | Nested objects (array) | Categories, tags |
| **CompletionField** | Autocomplete suggestions | Search suggestions as you type |

#### Step 5: Index Data

```bash
# Create Elasticsearch indices
python manage.py search_index --create

# Populate indices with existing data
python manage.py search_index --populate

# Rebuild indices (delete and recreate)
python manage.py search_index --rebuild

# Show index status
curl http://localhost:9200/_cat/indices?v
```

#### Step 6: Auto-indexing with Signals

Elasticsearch should stay in sync with database. This happens automatically with `django-elasticsearch-dsl`, but you can customize:

Create `books/signals.py`:

```python
from django.db.models.signals import post_save, post_delete
from django.dispatch import receiver
from django_elasticsearch_dsl.registries import registry


@receiver(post_save)
def update_document(sender, **kwargs):
    """
    Update Elasticsearch when model is saved.
    Automatically triggered by django-elasticsearch-dsl.
    """
    app_label = sender._meta.app_label
    model_name = sender.__name__
    instance = kwargs['instance']
    
    if app_label == 'books':
        # Update ES document
        registry.update(instance)


@receiver(post_delete)
def delete_document(sender, **kwargs):
    """Delete from Elasticsearch when model is deleted."""
    app_label = sender._meta.app_label
    model_name = sender.__name__
    instance = kwargs['instance']
    
    if app_label == 'books':
        registry.delete(instance, raise_on_error=False)
```

#### Step 7: Search ViewSet

Create `books/search_views.py`:

```python
from rest_framework import viewsets
from rest_framework.decorators import action
from rest_framework.response import Response
from elasticsearch_dsl import Q
from .documents import BookDocument
from .serializers import BookListSerializer


class BookSearchViewSet(viewsets.ViewSet):
    """
    Advanced search functionality with Elasticsearch.
    """
    
    document = BookDocument
    
    def list(self, request):
        """
        Full-text search with facets and filters.
        
        Query params:
        - q: Search query
        - category: Filter by category name
        - price_min: Minimum price
        - price_max: Maximum price
        - rating_min: Minimum average rating
        - sort: Sort field (price, rating, date, relevance)
        - page: Page number
        - size: Results per page
        
        Example: /api/search/?q=django&category=Programming&price_max=50&sort=rating
        """
        # Get query parameters
        query = request.query_params.get('q', '')
        category = request.query_params.get('category')
        price_min = request.query_params.get('price_min')
        price_max = request.query_params.get('price_max')
        rating_min = request.query_params.get('rating_min')
        sort = request.query_params.get('sort', 'relevance')
        page = int(request.query_params.get('page', 1))
        size = int(request.query_params.get('size', 20))
        
        # Build Elasticsearch query
        search = self.document.search()
        
        # 1. Full-text search query
        if query:
            q = Q(
                'multi_match',
                query=query,
                fields=[
                    'title^3',           # Title most important
                    'author.full_name^2', # Author second
                    'description',
                    'categories.name'
                ],
                fuzziness='AUTO',        # Handle typos
                operator='and',          # All words should match
                minimum_should_match='75%'  # At least 75% of words
            )
            search = search.query(q)
        else:
            # If no query, match all
            search = search.query('match_all')
        
        # 2. Filters (don't affect relevance score)
        filters = []
        
        if category:
            filters.append(Q('term', categories__name__raw=category))
        
        if price_min:
            filters.append(Q('range', price={'gte': float(price_min)}))
        
        if price_max:
            filters.append(Q('range', price={'lte': float(price_max)}))
        
        if rating_min:
            filters.append(Q('range', average_rating={'gte': float(rating_min)}))
        
        # Apply filters
        if filters:
            search = search.filter('bool', must=filters)
        
        # 3. Sorting
        if sort == 'price_asc':
            search = search.sort('price')
        elif sort == 'price_desc':
            search = search.sort('-price')
        elif sort == 'rating':
            search = search.sort('-average_rating')
        elif sort == 'date':
            search = search.sort('-publication_date')
        elif sort == 'popular':
            search = search.sort('-review_count')
        # else: sort by relevance (default)
        
        # 4. Aggregations (facets for filtering UI)
        search.aggs.bucket('categories', 'terms', field='categories.name.raw', size=20)
        search.aggs.bucket('price_ranges', 'range', field='price', ranges=[
            {'to': 10},
            {'from': 10, 'to': 25},
            {'from': 25, 'to': 50},
            {'from': 50}
        ])
        search.aggs.bucket('ratings', 'terms', field='average_rating')
        
        # 5. Pagination
        start = (page - 1) * size
        search = search[start:start + size]
        
        # 6. Execute search
        response = search.execute()
        
        # 7. Format results
        results = {
            'count': response.hits.total.value,
            'page': page,
            'size': size,
            'results': [self._format_hit(hit) for hit in response.hits],
            'facets': {
                'categories': [
                    {'name': bucket.key, 'count': bucket.doc_count}
                    for bucket in response.aggregations.categories.buckets
                ],
                'price_ranges': [
                    {
                        'range': f"${bucket.get('from', 0)}-${bucket.get('to', '+')}",
                        'count': bucket.doc_count
                    }
                    for bucket in response.aggregations.price_ranges.buckets
                ],
                'ratings': [
                    {'rating': bucket.key, 'count': bucket.doc_count}
                    for bucket in response.aggregations.ratings.buckets
                ]
            }
        }
        
        return Response(results)
    
    @action(detail=False, methods=['get'])
    def autocomplete(self, request):
        """
        Autocomplete suggestions as user types.
        
        GET /api/search/autocomplete/?q=djan
        Returns: ["Django for Beginners", "Django REST Framework", ...]
        """
        query = request.query_params.get('q', '')
        
        if len(query) < 2:
            return Response([])
        
        # Use completion suggester for fast autocomplete
        search = self.document.search()
        search = search.suggest(
            'title_suggest',
            query,
            completion={'field': 'title.suggest', 'size': 10}
        )
        
        response = search.execute()
        suggestions = [
            option.text
            for option in response.suggest.title_suggest[0].options
        ]
        
        return Response(suggestions)
    
    @action(detail=False, methods=['get'])
    def similar(self, request):
        """
        Find similar books using More Like This query.
        
        GET /api/search/similar/?book_id=123
        """
        book_id = request.query_params.get('book_id')
        
        if not book_id:
            return Response({'error': 'book_id required'}, status=400)
        
        # More Like This query
        search = self.document.search()
        search = search.query(
            'more_like_this',
            fields=['title', 'description', 'categories.name'],
            like=[{'_id': book_id}],
            min_term_freq=1,
            max_query_terms=12,
            min_doc_freq=1
        )
        
        search = search[:10]
        response = search.execute()
        
        results = [self._format_hit(hit) for hit in response.hits]
        return Response(results)
    
    def _format_hit(self, hit):
        """Format Elasticsearch hit as dictionary."""
        return {
            'id': hit.id,
            'title': hit.title,
            'author': hit.author.full_name if hasattr(hit, 'author') else None,
            'price': hit.price,
            'average_rating': hit.average_rating,
            'review_count': hit.review_count,
            'categories': [cat.name for cat in hit.categories] if hasattr(hit, 'categories') else [],
            'score': hit.meta.score,  # Relevance score
        }
```

**Search Features Explained**:

| Feature | What It Does | Example Use Case |
|---------|--------------|------------------|
| **multi_match** | Search multiple fields | Search in title, author, description simultaneously |
| **fuzziness** | Handle typos | "djnago" matches "django", "rest framwork" matches "rest framework" |
| **boost** | Weight field importance | Title matches rank higher than description matches |
| **filters** | Non-scored filtering | Price $10-$50, rating ‚â• 4 stars |
| **aggregations** | Count by category | Show "Programming (45), Fiction (23), ..." |
| **completion suggester** | Fast autocomplete | Suggestions appear as user types |
| **more_like_this** | Similarity search | "Customers who liked this also liked..." |

#### Step 8: Add to URLs

Update `books/urls.py`:

```python
from .search_views import BookSearchViewSet

router = DefaultRouter()
router.register(r'search', BookSearchViewSet, basename='search')
```

#### Step 9: Testing Elasticsearch

```bash
# Simple search
curl "http://localhost:8000/api/search/?q=django"

# Search with filters
curl "http://localhost:8000/api/search/?q=programming&price_max=50&category=Technology"

# Autocomplete
curl "http://localhost:8000/api/search/autocomplete/?q=djan"

# Similar books
curl "http://localhost:8000/api/search/similar/?book_id=123"
```

### 6.3 Elasticsearch Best Practices

#### DO's ‚úÖ

| Practice | Benefit | How |
|----------|---------|-----|
| **Use analyzers** | Better search results | Custom analyzers for different languages, domains |
| **Index relationships** | Faster queries | Denormalize data (copy author name into book document) |
| **Use filters over queries** | Better performance | Filters are cached, queries are not |
| **Implement pagination** | Handle large result sets | Never fetch all results, use from/size |
| **Monitor cluster health** | Prevent outages | Use Kibana, check cluster status regularly |
| **Use aliases** | Zero-downtime reindexing | Point alias to new index, switch atomically |

#### DON'Ts ‚ùå

| Anti-pattern | Problem | Solution |
|--------------|---------|----------|
| **Deep pagination** | Slow for large offsets | Use search_after for deep pagination |
| **Index everything** | Wasted resources | Only index searchable fields |
| **Ignore mapping** | Poor search quality | Define explicit mappings, don't rely on auto-mapping |
| **No replication** | Data loss risk | Set replicas=1+ in production |
| **Synchronous indexing** | Slow write operations | Use Celery tasks for bulk indexing |

---

## 7. Analytics Implementation

### 7.1 Why Analytics Matter

**Data without analytics** = Flying blind

**Example Metrics You Should Track**:

| Metric | Business Question | Action |
|--------|------------------|--------|
| **Daily Active Users (DAU)** | Is our user base growing? | If declining, investigate why |
| **API Response Time** | Is the API fast enough? | If >500ms, optimize queries |
| **Error Rate** | Is the API reliable? | If >1%, fix bugs |
| **Conversion Rate** | How many visitors become customers? | If low, improve UX |
| **Popular Books** | What do users like? | Stock more, promote |
| **Search Queries** | What are users looking for? | Improve search, add content |

### 7.2 Google Analytics Integration

#### Step 1: Get Tracking Code

1. Go to https://analytics.google.com
2. Create account and property
3. Get Measurement ID (looks like `G-XXXXXXXXXX`)

#### Step 2: Track API Events

Create `books/analytics.py`:

```python
import requests
from django.conf import settings


class GoogleAnalytics:
    """
    Send events to Google Analytics 4.
    
    Use for:
    - API usage tracking
    - User behavior analysis
    - Conversion tracking
    """
    
    def __init__(self):
        self.measurement_id = settings.GA_MEASUREMENT_ID
        self.api_secret = settings.GA_API_SECRET
        self.endpoint = f"https://www.google-analytics.com/mp/collect?measurement_id={self.measurement_id}&api_secret={self.api_secret}"
    
    def track_event(self, client_id, event_name, event_params=None):
        """
        Send event to Google Analytics.
        
        Args:
            client_id: Unique user identifier (user ID or session ID)
            event_name: Event name (e.g., 'book_view', 'purchase')
            event_params: Additional event data (dict)
        """
        payload = {
            'client_id': str(client_id),
            'events': [{
                'name': event_name,
                'params': event_params or {}
            }]
        }
        
        try:
            response = requests.post(self.endpoint, json=payload, timeout=5)
            response.raise_for_status()
        except requests.RequestException as e:
            # Don't fail API requests if analytics fails
            print(f"Analytics error: {e}")
    
    def track_page_view(self, client_id, page_path, page_title=None):
        """Track page/endpoint view."""
        self.track_event(
            client_id=client_id,
            event_name='page_view',
            event_params={
                'page_path': page_path,
                'page_title': page_title or page_path,
            }
        )
    
    def track_purchase(self, client_id, order_id, value, currency='USD', items=None):
        """Track purchase conversion."""
        self.track_event(
            client_id=client_id,
            event_name='purchase',
            event_params={
                'transaction_id': str(order_id),
                'value': float(value),
                'currency': currency,
                'items': items or []
            }
        )
    
    def track_search(self, client_id, search_term, results_count):
        """Track search queries."""
        self.track_event(
            client_id=client_id,
            event_name='search',
            event_params={
                'search_term': search_term,
                'results_count': results_count
            }
        )
```

#### Step 3: Add Analytics Middleware

Create `books/middleware/analytics.py`:

```python
from books.analytics import GoogleAnalytics


class AnalyticsMiddleware:
    """
    Track all API requests in Google Analytics.
    """
    
    def __init__(self, get_response):
        self.get_response = get_response
        self.ga = GoogleAnalytics()
    
    def __call__(self, request):
        # Process request
        response = self.get_response(request)
        
        # Track in background (don't slow down response)
        if request.path.startswith('/api/'):
            self._track_request(request, response)
        
        return response
    
    def _track_request(self, request, response):
        """Track API request."""
        # Get user identifier
        if request.user.is_authenticated:
            client_id = request.user.id
        else:
            # Use session key for anonymous users
            client_id = request.session.session_key or 'anonymous'
        
        # Track page view
        self.ga.track_page_view(
            client_id=client_id,
            page_path=request.path,
            page_title=f"{request.method} {request.path}"
        )
```

Add to `settings.py`:

```python
MIDDLEWARE = [
    # ... other middleware
    'books.middleware.analytics.AnalyticsMiddleware',
]

# Google Analytics credentials
GA_MEASUREMENT_ID = os.getenv('GA_MEASUREMENT_ID')
GA_API_SECRET = os.getenv('GA_API_SECRET')
```

#### Step 4: Track Custom Events

In your views:

```python
from books.analytics import GoogleAnalytics

class BookViewSet(viewsets.ModelViewSet):
    
    def retrieve(self, request, *args, **kwargs):
        """Get book detail."""
        book = self.get_object()
        
        # Track book view
        ga = GoogleAnalytics()
        ga.track_event(
            client_id=request.user.id if request.user.is_authenticated else 'anonymous',
            event_name='book_view',
            event_params={
                'book_id': book.id,
                'book_title': book.title,
                'book_price': float(book.price),
                'category': book.categories.first().name if book.categories.exists() else None
            }
        )
        
        return super().retrieve(request, *args, **kwargs)


class OrderViewSet(viewsets.ModelViewSet):
    
    @action(detail=False, methods=['post'])
    def create_from_cart(self, request):
        """Create order."""
        # ... order creation logic ...
        
        # Track purchase
        ga = GoogleAnalytics()
        ga.track_purchase(
            client_id=request.user.id,
            order_id=order.id,
            value=float(order.total_amount),
            items=[
                {
                    'item_id': str(item.book.id),
                    'item_name': item.book.title,
                    'price': float(item.price),
                    'quantity': item.quantity
                }
                for item in order.items.all()
            ]
        )
        
        return Response(OrderSerializer(order).data)
```

### 7.3 Custom Analytics with PostgreSQL

For detailed internal analytics, store events in your database.

#### Step 1: Create Event Model

Add to `books/models.py`:

```python
class AnalyticsEvent(models.Model):
    """Store analytics events for custom reporting."""
    
    EVENT_TYPES = [
        ('page_view', 'Page View'),
        ('book_view', 'Book View'),
        ('search', 'Search'),
        ('add_to_cart', 'Add to Cart'),
        ('purchase', 'Purchase'),
        ('review', 'Review'),
    ]
    
    user = models.ForeignKey(
        'auth.User',
        on_delete=models.SET_NULL,
        null=True,
        blank=True
    )
    session_id = models.CharField(max_length=255, blank=True)
    event_type = models.CharField(max_length=50, choices=EVENT_TYPES)
    event_data = models.JSONField(default=dict)
    timestamp = models.DateTimeField(auto_now_add=True)
    ip_address = models.GenericIPAddressField(null=True)
    user_agent = models.TextField(blank=True)
    
    class Meta:
        indexes = [
            models.Index(fields=['event_type', 'timestamp']),
            models.Index(fields=['user', 'timestamp']),
        ]
    
    def __str__(self):
        return f"{self.event_type} at {self.timestamp}"
```

#### Step 2: Analytics Service

Create `books/services/analytics.py`:

```python
from books.models import AnalyticsEvent
from django.db.models import Count, Avg, Sum
from django.utils import timezone
from datetime import timedelta


class AnalyticsService:
    """
    Custom analytics queries.
    """
    
    @staticmethod
    def track_event(request, event_type, event_data=None):
        """Track custom event."""
        AnalyticsEvent.objects.create(
            user=request.user if request.user.is_authenticated else None,
            session_id=request.session.session_key,
            event_type=event_type,
            event_data=event_data or {},
            ip_address=request.META.get('REMOTE_ADDR'),
            user_agent=request.META.get('HTTP_USER_AGENT', '')
        )
    
    @staticmethod
    def get_daily_active_users(days=30):
        """Get DAU over last N days."""
        since = timezone.now() - timedelta(days=days)
        
        return AnalyticsEvent.objects.filter(
            timestamp__gte=since,
            user__isnull=False
        ).values('timestamp__date').annotate(
            dau=Count('user', distinct=True)
        ).order_by('timestamp__date')
    
    @staticmethod
    def get_popular_books(days=7, limit=10):
        """Get most viewed books."""
        since = timezone.now() - timedelta(days=days)
        
        events = AnalyticsEvent.objects.filter(
            event_type='book_view',
            timestamp__gte=since
        )
        
        book_views = events.values(
            'event_data__book_id'
        ).annotate(
            views=Count('id')
        ).order_by('-views')[:limit]
        
        return book_views
    
    @staticmethod
    def get_search_queries(days=7, limit=20):
        """Get most popular search queries."""
        since = timezone.now() - timedelta(days=days)
        
        searches = AnalyticsEvent.objects.filter(
            event_type='search',
            timestamp__gte=since
        ).values(
            'event_data__query'
        ).annotate(
            count=Count('id')
        ).order_by('-count')[:limit]
        
        return searches
    
    @staticmethod
    def get_conversion_rate(days=30):
        """Calculate conversion rate (purchases / page views)."""
        since = timezone.now() - timedelta(days=days)
        
        page_views = AnalyticsEvent.objects.filter(
            event_type='page_view',
            timestamp__gte=since
        ).count()
        
        purchases = AnalyticsEvent.objects.filter(
            event_type='purchase',
            timestamp__gte=since
        ).count()
        
        if page_views == 0:
            return 0
        
        return (purchases / page_views) * 100
    
    @staticmethod
    def get_revenue_by_day(days=30):
        """Get daily revenue."""
        since = timezone.now() - timedelta(days=days)
        
        return AnalyticsEvent.objects.filter(
            event_type='purchase',
            timestamp__gte=since
        ).values('timestamp__date').annotate(
            revenue=Sum('event_data__total_amount')
        ).order_by('timestamp__date')
```

#### Step 3: Analytics Dashboard Endpoint

Add to `books/views.py`:

```python
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAdminUser
from books.services.analytics import AnalyticsService


@api_view(['GET'])
@permission_classes([IsAdminUser])
def analytics_dashboard(request):
    """
    Analytics dashboard for admins.
    
    GET /api/analytics/dashboard/
    """
    days = int(request.query_params.get('days', 30))
    
    data = {
        'daily_active_users': list(AnalyticsService.get_daily_active_users(days)),
        'popular_books': list(AnalyticsService.get_popular_books(7)),
        'search_queries': list(AnalyticsService.get_search_queries(7)),
        'conversion_rate': AnalyticsService.get_conversion_rate(days),
        'revenue_by_day': list(AnalyticsService.get_revenue_by_day(days)),
    }
    
    return Response(data)
```

### 7.4 Real-time Analytics with Redis

For real-time metrics (current active users, request rate), use Redis.

Create `books/services/realtime_analytics.py`:

```python
from django.core.cache import cache
from django.utils import timezone


class RealtimeAnalytics:
    """
    Real-time metrics using Redis.
    """
    
    @staticmethod
    def track_active_user(user_id):
        """Mark user as active (expires in 5 minutes)."""
        key = f'active_user:{user_id}'
        cache.set(key, timezone.now().isoformat(), 300)  # 5 minutes
    
    @staticmethod
    def get_active_users_count():
        """Get count of active users in last 5 minutes."""
        # In production, use Redis SCAN
        # This is simplified
        keys = cache.keys('active_user:*')
        return len(keys)
    
    @staticmethod
    def increment_counter(metric_name, value=1):
        """Increment a counter."""
        key = f'counter:{metric_name}'
        current = cache.get(key, 0)
        cache.set(key, current + value)
    
    @staticmethod
    def track_request_rate():
        """Track requests per minute."""
        now = timezone.now()
        minute_key = f'requests:{now.strftime("%Y%m%d%H%M")}'
        
        # Increment counter for this minute
        current = cache.get(minute_key, 0)
        cache.set(minute_key, current + 1, 120)  # Expire after 2 minutes
        
        return current + 1
```

---

## 8. Best Practices and Production Tips

### 8.1 Security Checklist

‚úÖ **Authentication & Authorization**
- [ ] Use JWT for mobile apps
- [ ] Implement rate limiting (100-1000 req/min)
- [ ] Add API key authentication for service-to-service calls
- [ ] Enable CORS only for trusted domains
- [ ] Use HTTPS in production (Let's Encrypt)

‚úÖ **Data Protection**
- [ ] Encrypt secrets (use HashiCorp Vault, AWS Secrets Manager)
- [ ] Sanitize user input (prevent XSS, SQL injection)
- [ ] Validate file uploads (type, size, malware scan)
- [ ] Implement data retention policies (GDPR compliance)
- [ ] Enable database encryption at rest

‚úÖ **Infrastructure**
- [ ] Use security groups/firewalls (block unnecessary ports)
- [ ] Regular security updates (OS, Python packages)
- [ ] Implement Web Application Firewall (WAF)
- [ ] Enable DDoS protection (Cloudflare, AWS Shield)
- [ ] Use private networks for internal services

### 8.2 Performance Optimization

#### Database
```python
# ‚ùå Bad: N+1 queries
books = Book.objects.all()
for book in books:
    print(book.author.name)  # New query for each book!

# ‚úÖ Good: One query with join
books = Book.objects.select_related('author').all()
for book in books:
    print(book.author.name)  # No additional query

# ‚ùå Bad: Loading all data
books = Book.objects.all()  # Loads all fields

# ‚úÖ Good: Only needed fields
books = Book.objects.only('id', 'title', 'price')

# ‚ùå Bad: No indexing
class Book(models.Model):
    isbn = models.CharField(max_length=13)  # No index!

# ‚úÖ Good: Add index
class Book(models.Model):
    isbn = models.CharField(max_length=13, db_index=True)
```

#### Caching Strategy

```python
# Level 1: View caching (cache entire response)
from django.views.decorators.cache import cache_page

@cache_page(60 * 15)  # Cache for 15 minutes
def book_list(request):
    ...

# Level 2: Query caching (cache database queries)
from django.core.cache import cache

def get_bestsellers():
    cache_key = 'bestsellers'
    result = cache.get(cache_key)
    
    if result is None:
        result = Book.objects.annotate(
            sales=Count('orders')
        ).order_by('-sales')[:10]
        cache.set(cache_key, result, 3600)  # 1 hour
    
    return result

# Level 3: Template fragment caching
{% load cache %}
{% cache 500 sidebar %}
    ... expensive sidebar rendering ...
{% endcache %}
```

### 8.3 Monitoring and Alerting

#### Metrics to Track

| Metric | Target | Alert If |
|--------|--------|----------|
| **Response Time (p95)** | < 200ms | > 500ms |
| **Error Rate** | < 0.1% | > 1% |
| **CPU Usage** | < 70% | > 85% |
| **Memory Usage** | < 80% | > 90% |
| **Database Connections** | < 80% of max | > 90% |
| **Queue Length** | < 100 | > 1000 |

#### Sentry Integration

```python
# settings.py
import sentry_sdk
from sentry_sdk.integrations.django import DjangoIntegration

sentry_sdk.init(
    dsn=os.getenv('SENTRY_DSN'),
    integrations=[DjangoIntegration()],
    traces_sample_rate=0.1,  # Sample 10% of transactions
    send_default_pii=False,  # Don't send PII
    environment=os.getenv('ENVIRONMENT', 'development'),
)

# Capture custom errors
try:
    risky_operation()
except Exception as e:
    sentry_sdk.capture_exception(e)
```

### 8.4 Deployment Checklist

Before going to production:

‚úÖ **Code Quality**
- [ ] All tests passing (>80% coverage)
- [ ] No linting errors
- [ ] Security scan passed
- [ ] Code reviewed by team

‚úÖ **Configuration**
- [ ] DEBUG=False
- [ ] SECRET_KEY is random and secure
- [ ] ALLOWED_HOSTS configured
- [ ] Database using PostgreSQL (not SQLite)
- [ ] Static files served by CDN
- [ ] Media files in cloud storage (S3)

‚úÖ **Performance**
- [ ] Database migrations tested
- [ ] Queries optimized (no N+1)
- [ ] Caching configured
- [ ] CDN configured
- [ ] GZIP compression enabled

‚úÖ **Monitoring**
- [ ] Logging configured
- [ ] Error tracking (Sentry)
- [ ] Performance monitoring (New Relic/DataDog)
- [ ] Uptime monitoring (Pingdom)
- [ ] Alerts configured

‚úÖ **Security**
- [ ] HTTPS enabled
- [ ] Security headers configured
- [ ] Rate limiting enabled
- [ ] Regular backups scheduled
- [ ] Disaster recovery plan documented

‚úÖ **Documentation**
- [ ] API documentation published
- [ ] Deployment runbook created
- [ ] Rollback procedure documented
- [ ] Team trained

---

## üéâ Conclusion

Congratulations! You've learned how to build an enterprise-grade Django REST Framework API with:

‚úÖ **CI/CD** - Automated testing and deployment with GitHub Actions, GitLab CI, Jenkins  
‚úÖ **Kubernetes** - Container orchestration for infinite scalability  
‚úÖ **API Gateway** - Kong and AWS API Gateway for routing, security, and rate limiting  
‚úÖ **Machine Learning** - Recommendation engine for personalized user experience  
‚úÖ **Elasticsearch** - Fast, powerful search with autocomplete and facets  
‚úÖ **Analytics** - Google Analytics and custom analytics for data-driven decisions  

### What You've Achieved

| Before | After | Impact |
|--------|-------|--------|
| Manual deployment | Automated CI/CD | **Deploy 10x more frequently** |
| Single server | Kubernetes cluster | **Handle 100x more traffic** |
| Direct API access | API Gateway | **Better security and control** |
| Generic recommendations | ML-powered | **2-3x higher conversion** |
| Slow database search | Elasticsearch | **10-100x faster search** |
| No insights | Comprehensive analytics | **Data-driven decisions** |

### The Journey Continues

Your API is now production-ready, but the journey doesn't end here:

üöÄ **Next Steps**:
1. **Mobile SDK** - Create client libraries for iOS/Android/React Native
2. **GraphQL Layer** - Add GraphQL for flexible frontend queries
3. **Serverless Functions** - Move event handlers to AWS Lambda
4. **Multi-region** - Deploy to multiple regions for global users
5. **AI Integration** - Add GPT for chatbots, content generation
6. **Blockchain** - Implement Web3 features if relevant
7. **IoT Integration** - Connect to IoT devices via MQTT

### Key Takeaways

**1. Automate Everything**
- Manual processes don't scale
- Automation reduces errors and saves time
- Invest in CI/CD early

**2. Design for Scale**
- Horizontal scaling > vertical scaling
- Stateless services are easier to scale
- Use caching aggressively

**3. Monitor Obsessively**
- You can't fix what you can't measure
- Set up monitoring before problems occur
- Alert on trends, not just thresholds

**4. Security First**
- Security is not optional
- Defense in depth (multiple layers)
- Regular security audits

**5. Keep Learning**
- Technology evolves rapidly
- Stay updated with new tools and practices
- Join communities, attend conferences

### Resources for Further Learning

üìö **Books**:
- *Designing Data-Intensive Applications* by Martin Kleppmann
- *Site Reliability Engineering* by Google
- *The Phoenix Project* by Gene Kim

üåê **Online Courses**:
- Kubernetes: kubernetes.io/docs/tutorials/
- Elasticsearch: www.elastic.co/training/
- Machine Learning: coursera.org/learn/machine-learning

üë• **Communities**:
- Django Discord: discord.gg/django
- Kubernetes Slack: slack.k8s.io
- r/django on Reddit

---

**Thank you for following this guide!** üôè

If you found this helpful:
- ‚≠ê Star the repository
- üì¢ Share with your team
- üêõ Report issues
- üí° Suggest improvements

**Built with ‚ù§Ô∏è by Phinehas Macharia**

---

*Remember: Building great APIs is a journey, not a destination. Start small, iterate fast, and always put your users first. Happy coding! üöÄ*
